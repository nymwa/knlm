{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a989d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence as pad\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "import random as rd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f5a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt') as f:\n",
    "    train_sents = [sent.strip().split() for sent in f]\n",
    "    train_sents = [sent for sent in train_sents]\n",
    "with open('valid.txt') as f:\n",
    "    valid_sents = [sent.strip().split() for sent in f]\n",
    "    \n",
    "word_count = defaultdict(int)\n",
    "for sent in train_sents:\n",
    "    for word in sent:\n",
    "        word_count[word] += 1\n",
    "word_count = [(key, value) for key, value in word_count.items()]\n",
    "word_count.sort(key = lambda x: -x[1])\n",
    "\n",
    "word_list = [word for word, freq in word_count if word != '<unk>']\n",
    "word_list = ['<pad>', '<eos>', '<unk>'] + word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b899e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.token_dict = {token: index for index, token in enumerate(tokens)}\n",
    "        self.pad = self.token_dict['<pad>']\n",
    "        self.eos = self.token_dict['<eos>']\n",
    "        self.unk = self.token_dict['<unk>']\n",
    "\n",
    "    def __contain__(self, x):\n",
    "        return x in self.token_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.tokens[x]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if x in self:\n",
    "            return self.token_dict[x]\n",
    "        return self.unk\n",
    "\n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, sents):\n",
    "        self.sents = sents\n",
    "        self.lengths = torch.tensor([len(sent) for sent in sents])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.sents[index]\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Batch:\n",
    "\n",
    "    def __init__(self, inputs, outputs = None, lengths = None):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[1]\n",
    "\n",
    "    def get_num_tokens(self):\n",
    "        return sum(self.lengths)\n",
    "\n",
    "    def cuda(self):\n",
    "        self.inputs = self.inputs.cuda()\n",
    "\n",
    "        if self.outputs is not None:\n",
    "            self.outputs = self.outputs.cuda()\n",
    "            \n",
    "        return self\n",
    "\n",
    "    \n",
    "class Collator:\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def make_tensors(self, batch):\n",
    "        inputs  = [torch.tensor([self.vocab.eos] + sent) for sent in batch]\n",
    "        outputs = [torch.tensor(sent + [self.vocab.eos]) for sent in batch]\n",
    "        lengths = [len(sent) + 1 for sent in batch]\n",
    "\n",
    "        inputs = pad(inputs, padding_value = self.vocab.pad)\n",
    "        outputs = pad(outputs, padding_value = -100)\n",
    "        return inputs, outputs, lengths\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        inputs, outputs, lengths = self.make_tensors(batch)\n",
    "        return Batch(inputs, outputs, lengths)\n",
    "    \n",
    "    \n",
    "class Sampler(torch.utils.data.Sampler):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset,\n",
    "            max_tokens):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.max_tokens = max_tokens\n",
    "        self.batches = None\n",
    "\n",
    "    def generate_batches(self):\n",
    "        indices = self.get_indices()\n",
    "        batches = []\n",
    "        batch = []\n",
    "        acc = 0\n",
    "        max_len = 0\n",
    "        for index in indices:\n",
    "            acc += 1\n",
    "            this_len = self.dataset.lengths[index]\n",
    "            max_len = max(max_len, this_len)\n",
    "            if (acc * max_len) > self.max_tokens:\n",
    "                batches.append(batch)\n",
    "                batch = [index]\n",
    "                acc = 1\n",
    "                max_len = this_len\n",
    "            else:\n",
    "                batch.append(index)\n",
    "        if batch:\n",
    "            batches.append(batch)\n",
    "        rd.shuffle(batches)\n",
    "        return batches\n",
    "\n",
    "    def init_batches(self):\n",
    "        if self.batches is None:\n",
    "            self.batches = self.generate_batches()\n",
    "\n",
    "    def __len__(self):\n",
    "        self.init_batches()\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.init_batches()\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "        self.terminate_batches()\n",
    "\n",
    "\n",
    "class FixedSampler(Sampler):\n",
    "\n",
    "    def get_indices(self):\n",
    "        if not hasattr(self, 'indices'):\n",
    "            indices = torch.arange(len(self.dataset))\n",
    "            indices = indices[self.dataset.lengths[indices].argsort(descending = True)]\n",
    "            self.indices = indices\n",
    "        return self.indices\n",
    "\n",
    "    def terminate_batches(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomSampler(Sampler):\n",
    "\n",
    "    def get_indices(self):\n",
    "        indices = torch.randperm(len(self.dataset))\n",
    "        indices = indices[self.dataset.lengths[indices].argsort(descending = True)]\n",
    "        return indices\n",
    "\n",
    "    def terminate_batches(self):\n",
    "        self.batches = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2646aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 4000\n",
    "vocab = Vocab(word_list)\n",
    "train_data = [[vocab(word) for word in sent] for sent in train_sents]\n",
    "valid_data = [[vocab(word) for word in sent] for sent in valid_sents]\n",
    "train_dataset = Dataset(train_data)\n",
    "valid_dataset = Dataset(valid_data)\n",
    "train_sampler = RandomSampler(train_dataset, max_tokens)\n",
    "valid_sampler = FixedSampler(valid_dataset, max_tokens)\n",
    "collator = Collator(vocab)\n",
    "train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_sampler = train_sampler,\n",
    "        collate_fn = collator)\n",
    "valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_sampler = valid_sampler,\n",
    "        collate_fn = collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef23726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers = 2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        x = self.embeddings(batch.inputs)\n",
    "        packed = pack(x, batch.lengths, enforce_sorted=False)\n",
    "        output, _ = self.lstm(packed)\n",
    "        x, _ = unpack(output)\n",
    "        x = self.dropout(x)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea7b2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "        self.clear_epoch()\n",
    "        self.clear_tmp()\n",
    "\n",
    "    def update(self, batch, loss, lr):\n",
    "        self.tmp_loss_list.append(loss)\n",
    "        self.tmp_wpb_list.append(batch.get_num_tokens())\n",
    "        self.tmp_spb_list.append(len(batch))\n",
    "        self.tmp_lr_list.append(lr)\n",
    "\n",
    "    def step_tmp(self):\n",
    "        loss = sum(self.tmp_loss_list) / len(self.tmp_loss_list)\n",
    "        wpb = sum(self.tmp_wpb_list)\n",
    "        spb = sum(self.tmp_spb_list)\n",
    "        lr = sum(self.tmp_lr_list) / len(self.tmp_lr_list)\n",
    "\n",
    "        self.loss_list.append(loss)\n",
    "        self.wpb_list.append(wpb)\n",
    "        self.spb_list.append(spb)\n",
    "        self.lr_list.append(lr)\n",
    "        return loss, wpb, spb, lr\n",
    "\n",
    "    def clear_tmp(self):\n",
    "        self.tmp_loss_list = []\n",
    "        self.tmp_wpb_list = []\n",
    "        self.tmp_spb_list = []\n",
    "        self.tmp_lr_list = []\n",
    "\n",
    "    def clear_epoch(self):\n",
    "        self.loss_list = []\n",
    "        self.wpb_list = []\n",
    "        self.spb_list = []\n",
    "        self.lr_list = []\n",
    "\n",
    "    def step_log(self, epoch, num_steps, grad = None):\n",
    "        loss, wpb, spb, lr = self.step_tmp()\n",
    "        self.clear_tmp()\n",
    "\n",
    "        line = '| {}-inner'.format(self.name)\n",
    "        line += ' | epoch {}, {}/{}'.format(\n",
    "                epoch,\n",
    "                len(self.spb_list),\n",
    "                num_steps)\n",
    "        line += ' | loss {:.4f}'.format(loss)\n",
    "        line += ' | lr {:.8f}'.format(lr)\n",
    "        if grad:\n",
    "            line += ' | grad {:.4f}'.format(grad)\n",
    "        line += ' | w/b {}'.format(wpb)\n",
    "        line += ' | s/b {}'.format(spb)\n",
    "\n",
    "        return line\n",
    "\n",
    "    def avg(self, lst):\n",
    "        num_examples = sum(self.spb_list)\n",
    "        return sum([n * x for n, x in zip(self.spb_list, lst)]) / num_examples\n",
    "\n",
    "    def epoch_log(self, epoch, num_steps = None):\n",
    "        line = '| {}'.format(self.name)\n",
    "        line += ' | epoch {}'.format(epoch)\n",
    "        line += ' | loss {:.4f}'.format(self.avg(self.loss_list))\n",
    "        line += ' | lr {:.8f}'.format(self.avg(self.lr_list))\n",
    "        line += ' | w/b {:.1f}'.format(self.avg(self.wpb_list))\n",
    "        line += ' | s/b {:.1f}'.format(self.avg(self.spb_list))\n",
    "        if num_steps is not None:\n",
    "            line += ' | steps {}'.format(num_steps)\n",
    "        self.clear_epoch()\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abeff22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinExpScheduler(optim.lr_scheduler.LambdaLR):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer,\n",
    "            warmup_steps,\n",
    "            last_epoch=-1):\n",
    "\n",
    "        def lr_lambda(step):\n",
    "            r = max(1e-8, step / warmup_steps)\n",
    "            return min(r, r ** -0.5)\n",
    "\n",
    "        super().__init__(\n",
    "                optimizer,\n",
    "                lr_lambda,\n",
    "                last_epoch = last_epoch)\n",
    "\n",
    "\n",
    "class Opter:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            lr,\n",
    "            max_grad_norm,\n",
    "            scheduler = 'constant',\n",
    "            warmup_steps = 0,\n",
    "            start_factor = 1.0/3,\n",
    "            weight_decay = 0.01):\n",
    "\n",
    "        self.model = model\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.total_grad_norm = None\n",
    "\n",
    "        self.optimizer = optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay)\n",
    "\n",
    "        if scheduler == 'constant':\n",
    "            self.scheduler = optim.lr_scheduler.ConstantLR(\n",
    "                    self.optimizer,\n",
    "                    total_iters = warmup_steps,\n",
    "                    factor = start_factor)\n",
    "        elif scheduler == 'linear':\n",
    "            self.scheduler = optim.lr_scheduler.LinearLR(\n",
    "                    self.optimizer,\n",
    "                    total_iters = warmup_steps,\n",
    "                    start_factor = start_factor)\n",
    "        elif scheduler == 'linexp':\n",
    "            self.scheduler = LinExpScheduler(\n",
    "                    self.optimizer,\n",
    "                    warmup_steps)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.scheduler.get_last_lr()[0]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        self.total_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(),\n",
    "                self.max_grad_norm)\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0240b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCalc:\n",
    "\n",
    "    def __init__(self, label_smoothing = 0.0):\n",
    "        self.train_criterion = nn.CrossEntropyLoss(\n",
    "            ignore_index = -100,\n",
    "            label_smoothing = label_smoothing)\n",
    "        self.valid_criterion = nn.CrossEntropyLoss(\n",
    "            ignore_index = -100)\n",
    "    \n",
    "    def set_trainer(self, trainer):\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def get_pred_and_target(self, batch):\n",
    "        batch.cuda()\n",
    "        pred = self.trainer.model(batch)\n",
    "        pred = pred.view(-1, pred.size(-1))\n",
    "        target = batch.outputs.view(-1)\n",
    "        return pred, target\n",
    "\n",
    "    def for_train(self, batch):\n",
    "        pred, target = self.get_pred_and_target(batch)\n",
    "        loss = self.train_criterion(pred, target)\n",
    "        return loss\n",
    "\n",
    "    def for_valid(self, batch):\n",
    "        pred, target = self.get_pred_and_target(batch)\n",
    "        loss = self.valid_criterion(pred, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dae9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            model,\n",
    "            opter,\n",
    "            losscalc,\n",
    "            max_epochs,\n",
    "            step_interval,\n",
    "            save_interval):\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.model = model\n",
    "        self.opter = opter\n",
    "        self.losscalc = losscalc\n",
    "        self.losscalc.set_trainer(self)\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.epoch = 0\n",
    "        self.step = 0\n",
    "        self.step_interval = step_interval\n",
    "        self.num_accum = 0\n",
    "        self.grad_to_init = True\n",
    "        self.save_interval = save_interval\n",
    "        self.train_accum = Accumulator('train')\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        num_steps = (self.num_accum + len(self.train_loader)) // self.step_interval\n",
    "        this_step = 0\n",
    "        for index, batch in enumerate(self.train_loader):\n",
    "\n",
    "            if self.grad_to_init:\n",
    "                self.opter.zero_grad()\n",
    "                self.grad_to_init = False\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = self.losscalc.for_train(batch)\n",
    "                loss_val = loss.item()\n",
    "                loss = loss / self.step_interval\n",
    "            self.opter.scaler.scale(loss).backward()\n",
    "            self.num_accum += 1\n",
    "            self.train_accum.update(batch, loss_val, self.opter.get_lr())\n",
    "\n",
    "            if self.num_accum == self.step_interval:\n",
    "                this_step += 1\n",
    "                self.step += 1\n",
    "                self.num_accum = 0\n",
    "                self.opter.step()\n",
    "                self.grad_to_init = True\n",
    "                self.train_accum.step_log(\n",
    "                    self.epoch,\n",
    "                    num_steps,\n",
    "                    grad = self.opter.total_grad_norm)\n",
    "\n",
    "        print(self.train_accum.epoch_log(self.epoch, num_steps))\n",
    "\n",
    "    def valid(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        accum = Accumulator('valid')\n",
    "        for index, batch in enumerate(self.valid_loader):\n",
    "            with torch.no_grad():\n",
    "                loss = self.losscalc.for_valid(batch)\n",
    "            accum.update(batch, loss.item(), self.opter.get_lr())\n",
    "        accum.step_log(self.epoch, len(self.valid_loader))\n",
    "        print(accum.epoch_log(self.epoch))\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        Path('checkpoints').mkdir(parents = True, exist_ok = True)\n",
    "        path = 'checkpoints/{}.pt'.format(self.epoch)\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        logger.info('| checkpoint | saved to {}'.format(path))\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.max_epochs):\n",
    "            self.epoch += 1\n",
    "            self.train()\n",
    "            self.valid()\n",
    "            if self.epoch % self.save_interval == 0:\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0482c034",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4361371\n",
      "| train | epoch 1 | loss 2.8723 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 1 | loss 2.5891 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 2 | loss 2.3786 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 2 | loss 2.5259 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 3 | loss 2.2855 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 3 | loss 2.4555 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 4 | loss 2.2182 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 4 | loss 2.4474 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 5 | loss 2.1808 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 5 | loss 2.3893 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 6 | loss 2.1383 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 6 | loss 2.3376 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 7 | loss 2.0950 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 7 | loss 2.3424 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 8 | loss 2.0682 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 8 | loss 2.3296 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 9 | loss 2.0308 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 9 | loss 2.3241 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 10 | loss 2.0045 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 10 | loss 2.3231 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 11 | loss 1.9691 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 11 | loss 2.3166 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 12 | loss 1.9346 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 12 | loss 2.3602 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 13 | loss 1.9003 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 13 | loss 2.3511 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 14 | loss 1.8678 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 14 | loss 2.3174 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 15 | loss 1.8364 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 15 | loss 2.3731 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 16 | loss 1.8045 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 16 | loss 2.3690 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 17 | loss 1.7717 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 17 | loss 2.3592 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 18 | loss 1.7435 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 18 | loss 2.3737 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 19 | loss 1.7115 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 19 | loss 2.3728 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 20 | loss 1.6770 | lr 0.00100000 | w/b 4354.9 | s/b 415.3 | steps 151\n",
      "| valid | epoch 20 | loss 2.3831 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "max_grad_norm = 1.0\n",
    "scheduler = 'constant'\n",
    "warmup_steps = 0\n",
    "start_factor = 1.0\n",
    "weight_decay = 0.01\n",
    "epochs = 20\n",
    "step_interval = 1\n",
    "save_interval = 100\n",
    "\n",
    "model = LSTMLM(len(vocab), 512, dropout = 0.3)\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "model.cuda()\n",
    "opter = Opter(model, lr, max_grad_norm, scheduler, warmup_steps, start_factor, weight_decay)\n",
    "losscalc = LossCalc()\n",
    "trainer = Trainer(train_loader, valid_loader, model, opter, losscalc, epochs, step_interval, save_interval)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3ee970f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.5454)\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "\n",
    "for sent in valid_data:\n",
    "    inputs = pad([torch.tensor([vocab.eos] + sent)])\n",
    "    batch = Batch(inputs, lengths = [len(sent) + 1])\n",
    "    batch = batch.cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch)\n",
    "    outputs = outputs.cpu()\n",
    "    probs = torch.softmax(outputs, dim = -1)\n",
    "    for index, prob in zip(sent + [vocab.eos], probs):\n",
    "        lst.append(prob[0][index])\n",
    "\n",
    "print(2 ** (-torch.log2(torch.tensor(lst))).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f386242e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 0.8652504682540894\n",
      "<eos> 0.8487547636032104\n",
      "? 0.8134161233901978\n",
      "kute 0.7881070375442505\n",
      "la 0.739115297794342\n",
      "\" 0.7279046177864075\n",
      "li 0.6980776190757751\n",
      "pu 0.6824248433113098\n",
      "e 0.6773284673690796\n",
      "seme 0.6601577401161194\n",
      "! 0.6579836010932922\n",
      ". 0.644507884979248\n",
      "o 0.6288620829582214\n",
      "musi 0.5886187553405762\n",
      "<proper> 0.5794374942779541\n",
      "sewi 0.5680385828018188\n",
      "palisa 0.5649554133415222\n",
      "wan 0.5514410138130188\n",
      "luka 0.5401892066001892\n",
      "loje 0.5367066264152527\n",
      "sina 0.5238322615623474\n",
      "mi 0.5195893049240112\n",
      "seli 0.5109198093414307\n",
      "tawa 0.5006988644599915\n",
      "ike 0.4930996596813202\n",
      "ona 0.4930773377418518\n",
      "suli 0.4618352949619293\n",
      "tan 0.4574577808380127\n",
      "linja 0.4525604248046875\n",
      "pi 0.4514349699020386\n",
      ", 0.4460424780845642\n",
      "lon 0.4423387944698334\n",
      "sona 0.442315012216568\n",
      "taso 0.4398246109485626\n",
      "ma 0.43766433000564575\n",
      "- 0.4323057234287262\n",
      "soweli 0.4288788437843323\n",
      "anu 0.425137460231781\n",
      "ni 0.4200000464916229\n",
      "nasa 0.40567561984062195\n",
      "ala 0.4035630524158478\n",
      "mute 0.3959139287471771\n",
      "pona 0.38539350032806396\n",
      "walo 0.38318178057670593\n",
      "meli 0.38158804178237915\n",
      "moku 0.381308376789093\n",
      "sitelen 0.37482720613479614\n",
      "supa 0.3702646791934967\n",
      "ale 0.36833158135414124\n",
      "telo 0.3657974898815155\n",
      "nasin 0.3639976978302002\n",
      "wawa 0.35124850273132324\n",
      "sama 0.3448405861854553\n",
      "tomo 0.34249237179756165\n",
      "tu 0.33999571204185486\n",
      "waso 0.3384585976600647\n",
      "kalama 0.33684995770454407\n",
      "ken 0.32570257782936096\n",
      "tenpo 0.32346343994140625\n",
      "sijelo 0.3215625286102295\n",
      "ante 0.32148656249046326\n",
      "lili 0.3194918930530548\n",
      "mije 0.31943315267562866\n",
      "kule 0.3183826208114624\n",
      "moli 0.31613609194755554\n",
      "lukin 0.3099495470523834\n",
      "kepeken 0.295456200838089\n",
      "suwi 0.2947879433631897\n",
      "suno 0.29330959916114807\n",
      "kasi 0.29035571217536926\n",
      "kama 0.2824724614620209\n",
      "uta 0.28131893277168274\n",
      "jo 0.27592557668685913\n",
      "ilo 0.2686355412006378\n",
      "nanpa 0.2652972936630249\n",
      "insa 0.26374050974845886\n",
      "nena 0.26199018955230713\n",
      "pali 0.26123514771461487\n",
      "sin 0.26092493534088135\n",
      "pilin 0.2525884211063385\n",
      "open 0.24761846661567688\n",
      "pini 0.2421244978904724\n",
      "weka 0.23641884326934814\n",
      "ijo 0.23011884093284607\n",
      "toki 0.22500351071357727\n",
      "pakala 0.22419190406799316\n",
      "len 0.22203372418880463\n",
      "mama 0.21877802908420563\n",
      "jan 0.21393169462680817\n",
      "pan 0.21272194385528564\n",
      "nimi 0.20947642624378204\n",
      "kili 0.2008092850446701\n",
      "mani 0.20006947219371796\n",
      "kon 0.18794679641723633\n",
      "lawa 0.18565045297145844\n",
      "sike 0.17978143692016602\n",
      "pana 0.17723670601844788\n",
      "wile 0.17663796246051788\n",
      "sinpin 0.1631470024585724\n",
      "lupa 0.1488221138715744\n",
      "noka 0.14805759489536285\n",
      "awen 0.14652496576309204\n",
      "lipu 0.14480629563331604\n",
      "mun 0.13555268943309784\n",
      "<pad> 0\n",
      "<unk> 0\n",
      "kin 0\n",
      "poka 0\n",
      "en 0\n",
      "utala 0\n",
      "kulupu 0\n",
      "lape 0\n",
      "pimeja 0\n",
      "esun 0\n",
      "olin 0\n",
      "poki 0\n",
      "anpa 0\n",
      "ali 0\n",
      "' 0\n",
      "a 0\n",
      "kiwen 0\n",
      "lete 0\n",
      "akesi 0\n",
      "jaki 0\n",
      "ko 0\n",
      "pipi 0\n",
      "monsi 0\n",
      "selo 0\n",
      "alasa 0\n",
      "<number> 0\n",
      "jelo 0\n",
      "laso 0\n",
      "kala 0\n",
      "mu 0\n",
      "oko 0\n",
      "unpa 0\n",
      "monsuta 0\n",
      "kipisi 0\n",
      ") 0\n",
      "( 0\n",
      "namako 0\n",
      "kijetesantakalu 0\n",
      "mulapisu 0\n",
      "; 0\n",
      "tonsi 0\n",
      "soko 0\n",
      "epiku 0\n",
      "lanpan 0\n",
      "misikeke 0\n",
      "~ 0\n",
      "kokosila 0\n",
      "leko 0\n",
      "& 0\n",
      "jasima 0\n",
      "# 0\n"
     ]
    }
   ],
   "source": [
    "lst = [list() for token in vocab]\n",
    "\n",
    "for sent in valid_data:\n",
    "    sent = [vocab.eos] + sent\n",
    "    inputs = pad([torch.tensor(sent)])\n",
    "    batch = Batch(inputs, lengths = [len(sent)])\n",
    "    batch = batch.cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch)\n",
    "    outputs = outputs.cpu()\n",
    "    probs = torch.softmax(outputs, dim = -1)\n",
    "    values, indices = torch.max(probs, dim = -1)\n",
    "    for v, i in zip(values, indices):\n",
    "        lst[i].append(v.item())\n",
    "\n",
    "res = []\n",
    "for index, token in enumerate(vocab):\n",
    "    if len(lst[index]) == 0:\n",
    "        a = 0\n",
    "    else:\n",
    "        a = torch.tensor(lst[index]).mean().item()\n",
    "    res.append((token, a))\n",
    "res.sort(key = lambda x: -x[1])\n",
    "\n",
    "for token, prob in res:\n",
    "    print(token, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95b9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
