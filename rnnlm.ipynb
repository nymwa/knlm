{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a989d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence as pad\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "import random as rd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f5a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt') as f:\n",
    "    train_sents = [sent.strip().split() for sent in f]\n",
    "    train_sents = [sent for sent in train_sents if len(sent) <= 80]\n",
    "with open('valid.txt') as f:\n",
    "    valid_sents = [sent.strip().split() for sent in f]\n",
    "    \n",
    "word_count = defaultdict(int)\n",
    "for sent in train_sents:\n",
    "    for word in sent:\n",
    "        word_count[word] += 1\n",
    "word_count = [(key, value) for key, value in word_count.items()]\n",
    "word_count.sort(key = lambda x: -x[1])\n",
    "\n",
    "word_list = [word for word, freq in word_count if word != '<unk>']\n",
    "word_list = ['<pad>', '<eos>', '<unk>'] + word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b899e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.token_dict = {token: index for index, token in enumerate(tokens)}\n",
    "        self.pad = self.token_dict['<pad>']\n",
    "        self.eos = self.token_dict['<eos>']\n",
    "        self.unk = self.token_dict['<unk>']\n",
    "\n",
    "    def __contain__(self, x):\n",
    "        return x in self.token_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.tokens[x]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if x in self:\n",
    "            return self.token_dict[x]\n",
    "        return self.unk\n",
    "\n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, sents):\n",
    "        self.sents = sents\n",
    "        self.lengths = torch.tensor([len(sent) for sent in sents])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.sents[index]\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Batch:\n",
    "\n",
    "    def __init__(self, inputs, outputs = None, lengths = None):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[1]\n",
    "\n",
    "    def get_num_tokens(self):\n",
    "        return sum(self.lengths)\n",
    "\n",
    "    def cuda(self):\n",
    "        self.inputs = self.inputs.cuda()\n",
    "\n",
    "        if self.outputs is not None:\n",
    "            self.outputs = self.outputs.cuda()\n",
    "            \n",
    "        return self\n",
    "\n",
    "    \n",
    "class Collator:\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def make_tensors(self, batch):\n",
    "        inputs  = [torch.tensor([self.vocab.eos] + sent) for sent in batch]\n",
    "        outputs = [torch.tensor(sent + [self.vocab.eos]) for sent in batch]\n",
    "        lengths = [len(sent) + 1 for sent in batch]\n",
    "\n",
    "        inputs = pad(inputs, padding_value = self.vocab.pad)\n",
    "        outputs = pad(outputs, padding_value = -100)\n",
    "        return inputs, outputs, lengths\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        inputs, outputs, lengths = self.make_tensors(batch)\n",
    "        return Batch(inputs, outputs, lengths)\n",
    "    \n",
    "    \n",
    "class Sampler(torch.utils.data.Sampler):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset,\n",
    "            max_tokens):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.max_tokens = max_tokens\n",
    "        self.batches = None\n",
    "\n",
    "    def generate_batches(self):\n",
    "        indices = self.get_indices()\n",
    "        batches = []\n",
    "        batch = []\n",
    "        acc = 0\n",
    "        max_len = 0\n",
    "        for index in indices:\n",
    "            acc += 1\n",
    "            this_len = self.dataset.lengths[index]\n",
    "            max_len = max(max_len, this_len)\n",
    "            if (acc * max_len) > self.max_tokens:\n",
    "                batches.append(batch)\n",
    "                batch = [index]\n",
    "                acc = 1\n",
    "                max_len = this_len\n",
    "            else:\n",
    "                batch.append(index)\n",
    "        if batch:\n",
    "            batches.append(batch)\n",
    "        rd.shuffle(batches)\n",
    "        return batches\n",
    "\n",
    "    def init_batches(self):\n",
    "        if self.batches is None:\n",
    "            self.batches = self.generate_batches()\n",
    "\n",
    "    def __len__(self):\n",
    "        self.init_batches()\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.init_batches()\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "        self.terminate_batches()\n",
    "\n",
    "\n",
    "class FixedSampler(Sampler):\n",
    "\n",
    "    def get_indices(self):\n",
    "        if not hasattr(self, 'indices'):\n",
    "            indices = torch.arange(len(self.dataset))\n",
    "            indices = indices[self.dataset.lengths[indices].argsort(descending = True)]\n",
    "            self.indices = indices\n",
    "        return self.indices\n",
    "\n",
    "    def terminate_batches(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomSampler(Sampler):\n",
    "\n",
    "    def get_indices(self):\n",
    "        indices = torch.randperm(len(self.dataset))\n",
    "        indices = indices[self.dataset.lengths[indices].argsort(descending = True)]\n",
    "        return indices\n",
    "\n",
    "    def terminate_batches(self):\n",
    "        self.batches = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2646aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 4000\n",
    "vocab = Vocab(word_list)\n",
    "train_data = [[vocab(word) for word in sent] for sent in train_sents]\n",
    "valid_data = [[vocab(word) for word in sent] for sent in valid_sents]\n",
    "train_dataset = Dataset(train_data)\n",
    "valid_dataset = Dataset(valid_data)\n",
    "train_sampler = RandomSampler(train_dataset, max_tokens)\n",
    "valid_sampler = FixedSampler(valid_dataset, max_tokens)\n",
    "collator = Collator(vocab)\n",
    "train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_sampler = train_sampler,\n",
    "        collate_fn = collator)\n",
    "valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_sampler = valid_sampler,\n",
    "        collate_fn = collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef23726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        x = self.embeddings(batch.inputs)\n",
    "        packed = pack(x, batch.lengths, enforce_sorted=False)\n",
    "        output, _ = self.rnn(packed)\n",
    "        x, _ = unpack(output)\n",
    "        x = self.dropout(x)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea7b2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "        self.clear_epoch()\n",
    "        self.clear_tmp()\n",
    "\n",
    "    def update(self, batch, loss, lr):\n",
    "        self.tmp_loss_list.append(loss)\n",
    "        self.tmp_wpb_list.append(batch.get_num_tokens())\n",
    "        self.tmp_spb_list.append(len(batch))\n",
    "        self.tmp_lr_list.append(lr)\n",
    "\n",
    "    def step_tmp(self):\n",
    "        loss = sum(self.tmp_loss_list) / len(self.tmp_loss_list)\n",
    "        wpb = sum(self.tmp_wpb_list)\n",
    "        spb = sum(self.tmp_spb_list)\n",
    "        lr = sum(self.tmp_lr_list) / len(self.tmp_lr_list)\n",
    "\n",
    "        self.loss_list.append(loss)\n",
    "        self.wpb_list.append(wpb)\n",
    "        self.spb_list.append(spb)\n",
    "        self.lr_list.append(lr)\n",
    "        return loss, wpb, spb, lr\n",
    "\n",
    "    def clear_tmp(self):\n",
    "        self.tmp_loss_list = []\n",
    "        self.tmp_wpb_list = []\n",
    "        self.tmp_spb_list = []\n",
    "        self.tmp_lr_list = []\n",
    "\n",
    "    def clear_epoch(self):\n",
    "        self.loss_list = []\n",
    "        self.wpb_list = []\n",
    "        self.spb_list = []\n",
    "        self.lr_list = []\n",
    "\n",
    "    def step_log(self, epoch, num_steps, grad = None):\n",
    "        loss, wpb, spb, lr = self.step_tmp()\n",
    "        self.clear_tmp()\n",
    "\n",
    "        line = '| {}-inner'.format(self.name)\n",
    "        line += ' | epoch {}, {}/{}'.format(\n",
    "                epoch,\n",
    "                len(self.spb_list),\n",
    "                num_steps)\n",
    "        line += ' | loss {:.4f}'.format(loss)\n",
    "        line += ' | lr {:.8f}'.format(lr)\n",
    "        if grad:\n",
    "            line += ' | grad {:.4f}'.format(grad)\n",
    "        line += ' | w/b {}'.format(wpb)\n",
    "        line += ' | s/b {}'.format(spb)\n",
    "\n",
    "        return line\n",
    "\n",
    "    def avg(self, lst):\n",
    "        num_examples = sum(self.spb_list)\n",
    "        return sum([n * x for n, x in zip(self.spb_list, lst)]) / num_examples\n",
    "\n",
    "    def epoch_log(self, epoch, num_steps = None):\n",
    "        line = '| {}'.format(self.name)\n",
    "        line += ' | epoch {}'.format(epoch)\n",
    "        line += ' | loss {:.4f}'.format(self.avg(self.loss_list))\n",
    "        line += ' | lr {:.8f}'.format(self.avg(self.lr_list))\n",
    "        line += ' | w/b {:.1f}'.format(self.avg(self.wpb_list))\n",
    "        line += ' | s/b {:.1f}'.format(self.avg(self.spb_list))\n",
    "        if num_steps is not None:\n",
    "            line += ' | steps {}'.format(num_steps)\n",
    "        self.clear_epoch()\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abeff22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinExpScheduler(optim.lr_scheduler.LambdaLR):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer,\n",
    "            warmup_steps,\n",
    "            last_epoch=-1):\n",
    "\n",
    "        def lr_lambda(step):\n",
    "            r = max(1e-8, step / warmup_steps)\n",
    "            return min(r, r ** -0.5)\n",
    "\n",
    "        super().__init__(\n",
    "                optimizer,\n",
    "                lr_lambda,\n",
    "                last_epoch = last_epoch)\n",
    "\n",
    "\n",
    "class Opter:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            lr,\n",
    "            max_grad_norm,\n",
    "            scheduler = 'constant',\n",
    "            warmup_steps = 0,\n",
    "            start_factor = 1.0/3,\n",
    "            weight_decay = 0.01):\n",
    "\n",
    "        self.model = model\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.total_grad_norm = None\n",
    "\n",
    "        self.optimizer = optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay)\n",
    "\n",
    "        if scheduler == 'constant':\n",
    "            self.scheduler = optim.lr_scheduler.ConstantLR(\n",
    "                    self.optimizer,\n",
    "                    total_iters = warmup_steps,\n",
    "                    factor = start_factor)\n",
    "        elif scheduler == 'linear':\n",
    "            self.scheduler = optim.lr_scheduler.LinearLR(\n",
    "                    self.optimizer,\n",
    "                    total_iters = warmup_steps,\n",
    "                    start_factor = start_factor)\n",
    "        elif scheduler == 'linexp':\n",
    "            self.scheduler = LinExpScheduler(\n",
    "                    self.optimizer,\n",
    "                    warmup_steps)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.scheduler.get_last_lr()[0]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        self.total_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(),\n",
    "                self.max_grad_norm)\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0240b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCalc:\n",
    "\n",
    "    def __init__(self, label_smoothing = 0.0):\n",
    "        self.train_criterion = nn.CrossEntropyLoss(\n",
    "            ignore_index = -100,\n",
    "            label_smoothing = label_smoothing)\n",
    "        self.valid_criterion = nn.CrossEntropyLoss(\n",
    "            ignore_index = -100)\n",
    "    \n",
    "    def set_trainer(self, trainer):\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def get_pred_and_target(self, batch):\n",
    "        batch.cuda()\n",
    "        pred = self.trainer.model(batch)\n",
    "        pred = pred.view(-1, pred.size(-1))\n",
    "        target = batch.outputs.view(-1)\n",
    "        return pred, target\n",
    "\n",
    "    def for_train(self, batch):\n",
    "        pred, target = self.get_pred_and_target(batch)\n",
    "        loss = self.train_criterion(pred, target)\n",
    "        return loss\n",
    "\n",
    "    def for_valid(self, batch):\n",
    "        pred, target = self.get_pred_and_target(batch)\n",
    "        loss = self.valid_criterion(pred, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dae9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            model,\n",
    "            opter,\n",
    "            losscalc,\n",
    "            max_epochs,\n",
    "            step_interval,\n",
    "            save_interval):\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.model = model\n",
    "        self.opter = opter\n",
    "        self.losscalc = losscalc\n",
    "        self.losscalc.set_trainer(self)\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.epoch = 0\n",
    "        self.step = 0\n",
    "        self.step_interval = step_interval\n",
    "        self.num_accum = 0\n",
    "        self.grad_to_init = True\n",
    "        self.save_interval = save_interval\n",
    "        self.train_accum = Accumulator('train')\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        num_steps = (self.num_accum + len(self.train_loader)) // self.step_interval\n",
    "        this_step = 0\n",
    "        for index, batch in enumerate(self.train_loader):\n",
    "\n",
    "            if self.grad_to_init:\n",
    "                self.opter.zero_grad()\n",
    "                self.grad_to_init = False\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = self.losscalc.for_train(batch)\n",
    "                loss_val = loss.item()\n",
    "                loss = loss / self.step_interval\n",
    "            self.opter.scaler.scale(loss).backward()\n",
    "            self.num_accum += 1\n",
    "            self.train_accum.update(batch, loss_val, self.opter.get_lr())\n",
    "\n",
    "            if self.num_accum == self.step_interval:\n",
    "                this_step += 1\n",
    "                self.step += 1\n",
    "                self.num_accum = 0\n",
    "                self.opter.step()\n",
    "                self.grad_to_init = True\n",
    "                self.train_accum.step_log(\n",
    "                    self.epoch,\n",
    "                    num_steps,\n",
    "                    grad = self.opter.total_grad_norm)\n",
    "\n",
    "        print(self.train_accum.epoch_log(self.epoch, num_steps))\n",
    "\n",
    "    def valid(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        accum = Accumulator('valid')\n",
    "        for index, batch in enumerate(self.valid_loader):\n",
    "            with torch.no_grad():\n",
    "                loss = self.losscalc.for_valid(batch)\n",
    "            accum.update(batch, loss.item(), self.opter.get_lr())\n",
    "        accum.step_log(self.epoch, len(self.valid_loader))\n",
    "        print(accum.epoch_log(self.epoch))\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        Path('checkpoints').mkdir(parents = True, exist_ok = True)\n",
    "        path = 'checkpoints/{}.pt'.format(self.epoch)\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        logger.info('| checkpoint | saved to {}'.format(path))\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.max_epochs):\n",
    "            self.epoch += 1\n",
    "            self.train()\n",
    "            self.valid()\n",
    "            if self.epoch % self.save_interval == 0:\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0482c034",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211099\n",
      "| train | epoch 1 | loss 2.8736 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 1 | loss 2.6767 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 2 | loss 2.5067 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 2 | loss 2.6614 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 3 | loss 2.4416 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 3 | loss 2.6031 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 4 | loss 2.3937 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 4 | loss 2.5777 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 5 | loss 2.3680 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 5 | loss 2.5344 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 6 | loss 2.3470 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 6 | loss 2.5509 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 7 | loss 2.3296 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 7 | loss 2.5416 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 8 | loss 2.3140 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 8 | loss 2.5541 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 9 | loss 2.3094 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 9 | loss 2.5022 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 10 | loss 2.2978 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 10 | loss 2.5069 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 11 | loss 2.2870 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 11 | loss 2.5158 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 12 | loss 2.2838 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 12 | loss 2.5005 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 13 | loss 2.2744 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 13 | loss 2.4809 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 14 | loss 2.2735 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 14 | loss 2.5029 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 15 | loss 2.2654 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 15 | loss 2.5012 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 16 | loss 2.2586 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 16 | loss 2.4832 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 17 | loss 2.2502 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 17 | loss 2.4864 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 18 | loss 2.2487 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 18 | loss 2.4918 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 19 | loss 2.2500 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 19 | loss 2.5148 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 20 | loss 2.2448 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 20 | loss 2.4574 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 21 | loss 2.2377 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 21 | loss 2.4662 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 22 | loss 2.2337 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 22 | loss 2.4476 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 23 | loss 2.2320 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 23 | loss 2.4847 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 24 | loss 2.2308 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 24 | loss 2.4675 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 25 | loss 2.2267 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 25 | loss 2.4460 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 26 | loss 2.2216 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 26 | loss 2.4997 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 27 | loss 2.2245 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 27 | loss 2.4495 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 28 | loss 2.2184 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 28 | loss 2.4556 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 29 | loss 2.2139 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 29 | loss 2.4358 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 30 | loss 2.2132 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 30 | loss 2.4698 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 31 | loss 2.2212 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 31 | loss 2.4477 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 32 | loss 2.2047 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 32 | loss 2.4525 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 33 | loss 2.2066 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 33 | loss 2.4841 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 34 | loss 2.2072 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 34 | loss 2.4337 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 35 | loss 2.2029 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 35 | loss 2.4479 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 36 | loss 2.2055 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 36 | loss 2.4513 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 37 | loss 2.2040 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 37 | loss 2.4294 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 38 | loss 2.2002 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 38 | loss 2.4567 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 39 | loss 2.1995 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 39 | loss 2.4665 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 40 | loss 2.1933 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 40 | loss 2.4474 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 41 | loss 2.1959 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 41 | loss 2.4331 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 42 | loss 2.1906 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 42 | loss 2.4745 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 43 | loss 2.1958 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 43 | loss 2.4462 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 44 | loss 2.1917 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 44 | loss 2.4495 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 45 | loss 2.1914 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 45 | loss 2.4301 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 46 | loss 2.1875 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 46 | loss 2.4583 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 47 | loss 2.1845 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 47 | loss 2.4485 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 48 | loss 2.1843 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 48 | loss 2.4383 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 49 | loss 2.1815 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 49 | loss 2.4416 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n",
      "| train | epoch 50 | loss 2.1785 | lr 0.00100000 | w/b 4377.4 | s/b 420.5 | steps 146\n",
      "| valid | epoch 50 | loss 2.4429 | lr 0.00100000 | w/b 3490.0 | s/b 400.0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "max_grad_norm = 1.0\n",
    "scheduler = 'constant'\n",
    "warmup_steps = 0\n",
    "start_factor = 1.0\n",
    "weight_decay = 0.01\n",
    "epochs = 50\n",
    "step_interval = 1\n",
    "save_interval = 100\n",
    "\n",
    "model = RNNLM(len(vocab), 256, dropout = 0.2)\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "model.cuda()\n",
    "opter = Opter(model, lr, max_grad_norm, scheduler, warmup_steps, start_factor, weight_decay)\n",
    "losscalc = LossCalc()\n",
    "trainer = Trainer(train_loader, valid_loader, model, opter, losscalc, epochs, step_interval, save_interval)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3ee970f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8266)\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "\n",
    "for sent in valid_data:\n",
    "    sent = [vocab.eos] + sent\n",
    "    inputs = pad([torch.tensor(sent)])\n",
    "    batch = Batch(inputs, lengths = [len(sent)])\n",
    "    batch = batch.cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch)\n",
    "    outputs = outputs.cpu()\n",
    "    probs = torch.softmax(outputs, dim = -1)\n",
    "    values, indices = torch.max(probs, dim = -1)\n",
    "    for prob in values:\n",
    "        lst.append(prob)\n",
    "\n",
    "print(2 ** (-torch.log2(torch.tensor(lst))).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386242e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
